{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SAAF Notebook\n",
    "\n",
    "This Jupyter Notebook provides an interactive platform for FaaS development. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Notebook Setup\n",
    "\n",
    "Welcome to the SAAF Jupyter notebook! This default notebook provides comments to guide you through all of the main features. If you run into errors or probls please make sure you have the AWS CLI properly configure so that you can deploy function with it, have Docker installed and running, gave execute permission to everything in the /jupyter_workspace and /test directory, and finally installed all the dependencies. You can use quickInstall.sh in the root folder to walk you through the setup process and install dependencies. This notebook is designed to be ran locally or on EC2 instances and works very will with Visual Studio Code's Jupyeter plugin. Other environments may work but getting this notebook to work on cloud based platforms like Google Collab may be very difficult.\n",
    "\n",
    "Anyway, this first cell is just imports needed to setup the magic that goes on behind the scenes. Run it and it should return nothing. In this cell we define our config object, this object contains any information that we need to deploy functions, such as a role for functions on AWS Lambda. If all of your functions will use the same config object, you can set it globally by using setGlobalConfig. Any methods that take a config object will priorize the object passed to them over the global config.\n",
    "\n",
    "The setGlobalDeploy function defines that you want your cloud functions to be automatically deployed when they are ran. This can be disabled by setting the method to false.\n",
    "\n",
    "Function documentation available in jupyter_workspace/platforms/jupyter/interactive_helpers.py\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "from platforms.jupyter.interactive_helpers import *\n",
    "\n",
    "# Configure your function details here. Currently the only thing you need is a lambda ARN to assign to functions.\n",
    "config = {\n",
    "    \"lambdaRoleARN\": \"{FILL THIS IN}\"\n",
    "}\n",
    "setGlobalConfig(config)\n",
    "\n",
    "# If you want to disable automatic deployment across the entire notebook change this.\n",
    "setGlobalDeploy(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "\n",
    "Any function with the @cloud_function decorator will be uploaded to the cloud. Define platforms and memory settings in the decorator. \n",
    "Functions are tested locally and must run sucessfully before being deployed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Deploying Functions\n",
    "\n",
    "Here is your first cloud function! Creating cloud functions is as simple as writing python functions with (request, context) arguments and adding the @cloud_function decorator! See the two hello world functions below, they are nearly identical! But when we run them we will see that the CPU used on the cloud will be different than our local CPU returned by the SAAF inspector inspectCPU method. That is because the function is running on AWS Lambda! \n",
    "\n",
    "You can add arguments to the cloud_function dectorator to define the platform you would like to deploy to, the memory setting, and different context objects. Other arguments like references, requirements, and containerize can be used to change behavior.\n",
    "\n",
    "Cloud functions defined in this notebook do have a few limitations. The main one is that nothing outside the function is deployed to the cloud. That is why imports are inside the function, which is a little weird and can have an effect on what you can import. But for most things this is fine. \n",
    "\n",
    "Alongside deploying your function code, you can deploy files alongside this function by adding them to the src/includes_{function name} folder (This function will use src/includes_hello_world). This folder will be automatically created when the function is ran. You can include basically anything, files, scripts, python libraries, whatever you need.\n",
    "\n",
    "If everything is setup correct, all you need to do is run this code block and you'll get a hello_world function on AWS Lambda! If not all dependencies are installed you can use ./quickInstall.sh to download them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@cloud_function()\n",
    "def hello_world(request, context): \n",
    "    from Inspector import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectCPU()\n",
    "    inspector.addAttribute(\"message\", \"Hello from the cloud \" + str(request[\"name\"]) + \"!\")\n",
    "    return inspector.finish()\n",
    "\n",
    "def hello_world_local(request, context): \n",
    "    from Inspector import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectCPU()\n",
    "    inspector.addAttribute(\"message\", \"Hello from your computer \" + str(request[\"name\"]) + \"!\") \n",
    "    return inspector.finish()\n",
    "\n",
    "# Run our local hello_world function and check the CPU.\n",
    "local = hello_world_local({\"name\": \"Steve\"}, None)\n",
    "print(\"Local CPU: \" + local['cpuType'])\n",
    "\n",
    "# Run our cloud hello_world function and check the CPU.\n",
    "cloud = hello_world({\"name\": \"Steve\"}, None)\n",
    "print(\"Cloud CPU: \" + cloud['cpuType'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Chaining Functions and Run Modes\n",
    "\n",
    "What if we want one cloud function to call another function? This jello_world cloud function is calling the hello_world cloud function we created earlier. What is going to happen? When deployed, could will be generated automatically so that the jello_world function will make a request and call the hello_world function! Simply add any cloud functions that this function calls to the references list and this code will be generated.\n",
    "\n",
    "This function isn't cheating and just deploying both hello_world and jello_world together, both are deployed as seperate functions and making requests to the other. This example isn't practical but all features of python, such as multithreading, can be used to make multiple requests to functions in parallel. After running, see src/handler_jello_world.py for the automatically generated source code.\n",
    "\n",
    "Alongside that, this function has a custom run mode. There are three run modes that define how functions are executed when they are called. By default, RunMode.CLOUD is used and calling cloud functions will run them on the cloud. RunMode.LOCAL makes it so that cloud functions are executed locally when called on their own, so to run them on the cloud you must use the test method. As you can see here, we have one single function but like in the previous example we can see different CPUs depending on if it is ran locally or on the cloud using the test method. But, since hello_world is still a cloud function with the default RunMode.CLOUD, it will be called on the cloud instead of running locally. Finally, if you don't want your functions running locally or on the cloud but instead just deployed when the cell is ran you can use RunMode.NONE."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@cloud_function(references=[hello_world], runMode=RunMode.LOCAL)\n",
    "def jello_world(request, context): \n",
    "    from Inspector import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectAll()\n",
    "    \n",
    "    cloud_request = hello_world(request, None)\n",
    "    hello_message = cloud_request['message']\n",
    "    jello_message = hello_message.replace(\"Hello\", \"Jello\")\n",
    "    inspector.addAttribute(\"message\", jello_message)\n",
    "    inspector.addAttribute(\"cloud_request\", cloud_request)\n",
    "    \n",
    "    inspector.inspectAllDeltas()\n",
    "    return inspector.finish()\n",
    "\n",
    "\n",
    "local = jello_world({\"name\": \"Bob\"}, None)\n",
    "print(\"---\")\n",
    "print(\"Local jello_world CPU: \" + local['cpuType'])\n",
    "print(\"Local hello_world call in jello_world CPU: \" + local['cloud_request']['cpuType'])\n",
    "\n",
    "cloud = test(function=jello_world, payload={\"name\": \"Bob\"}, quiet=True, skipLocal=True)\n",
    "print(\"---\")\n",
    "print(\"Cloud jello_world CPU: \" + cloud['cpuType'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4: Requirements and Containerize\n",
    "\n",
    "This function here requires the igraph dependency, you can see it defined in the requirements argument of the decorator. Alongside that, this function will be deployed as a container instead of zip file. Containers are built, submitted to ECR, and deployed to AWS Lambda. For all function builds, you can see the generated files in the /deploy directory. The complete build for this function will be in /deploy/graph_rank_container_aws_build where you will be able to see all the python files, dependencies, and Dockerfile. The build folder will be destroyed and recreated every time a function is deployed so it is not recommended to manually edit. \n",
    "\n",
    "If the run mode was set to local, any dependencies this function uses would need to be install locally first. But since this function uses the default CLOUD run mode you do not need to install them.\n",
    "\n",
    "This function also uses more memory than the others, so we have changed the memory setting to 1024MBs instead of the default 256MBs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@cloud_function(memory=1024, requirements=\"python-igraph\", containerize=True)\n",
    "def page_rank_container(request, context):\n",
    "    from Inspector import Inspector \n",
    "    import datetime\n",
    "    import igraph\n",
    "    import time\n",
    "    \n",
    "    inspector = Inspector()\n",
    "    inspector.inspectAll()\n",
    "    \n",
    "    size = request.get('size')  \n",
    "    loops = request.get('loops')\n",
    "\n",
    "    for x in range(loops):\n",
    "        graph = igraph.Graph.Tree(size, 10)\n",
    "        result = graph.pagerank()  \n",
    "\n",
    "    inspector.inspectAllDeltas()\n",
    "    return inspector.finish()\n",
    "\n",
    "page_rank_container({\"size\": 10000, \"loops\": 5}, None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execute Experiments\n",
    "\n",
    "Use FaaS Runner to execute complex FaaS Experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 5: FaaS Runner Experiments\n",
    "\n",
    "Now, what's cooler than running a function on the cloud once? Running it multiple times! The run_experiment function allows you to create complex FaaS experiments. This function uses our FaaS Runner application to execute functions behind the scenes. It's primary purpose is to run multiple function requests across many threads. You define payloads in the payloads list, choose your memory setting (it will switch settings automatically) and define how many runs you want to do, across how many threads, and how many times you want to repeat the test with iterations. These are the most important parameters, but there are many more defined in the link below. \n",
    "\n",
    "After an experiment runs, the results are converted into a pandas dataframe that you can continue using in this notebook. For example you can use matplotlib to generate graphs (see below), or do any other form of data processing. \n",
    "\n",
    "Below are two different experiments for our functions. Execute them and generate graphs using the code cells below. You now have experienced all the functionality of the SAAF Jupyter Workspace! Happy FaaS developing!\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Define experiment parameters. For more detail see: https://github.com/wlloyduw/SAAF/tree/master/test\n",
    "hello_experiment = {\n",
    "  \"payloads\": [{\"name\": \"Bob\"}],\n",
    "  \"memorySettings\": [256],\n",
    "  \"runs\": 25,\n",
    "  \"threads\": 5,\n",
    "  \"iterations\": 4,\n",
    "  \"warmupBuffer\": 0,\n",
    "  \"sleepTime\": 0,\n",
    "  \"randomSeed\": 42,\n",
    "  \"showAsList\": [],\n",
    "  \"showAsSum\": [\"newcontainer\"],\n",
    "  \"ignoreFromAll\": [\"zAll\", \"version\", \"linuxVersion\", \"hostname\"],\n",
    "  \"invalidators\": {},\n",
    "  \"removeDuplicateContainers\": False,\n",
    "  \"overlapFilter\": \"functionName\",\n",
    "  \"openCSV\": False\n",
    "}\n",
    "\n",
    "# Execute experiment\n",
    "hello_world_results = run_experiment(function=hello_world, experiment=hello_experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_rank_experiment = {\n",
    "  \"payloads\": [{\"size\": 50000, \"loops\": 5},\n",
    "                {\"size\": 100000, \"loops\": 5},\n",
    "                {\"size\": 150000, \"loops\": 5}],\n",
    "  \"memorySettings\": [512],\n",
    "  \"runs\": 100,\n",
    "  \"threads\": 100,\n",
    "  \"iterations\": 2,\n",
    "  \"warmupBuffer\": 1,\n",
    "  \"sleepTime\": 0,\n",
    "  \"randomSeed\": 42,\n",
    "  \"showAsList\": [],\n",
    "  \"showAsSum\": [\"newcontainer\"],\n",
    "  \"ignoreFromAll\": [\"zAll\", \"version\", \"linuxVersion\", \"hostname\"],\n",
    "  \"invalidators\": {},\n",
    "  \"removeDuplicateContainers\": False,\n",
    "  \"overlapFilter\": \"functionName\",\n",
    "  \"openCSV\": True\n",
    "}\n",
    "\n",
    "# Execute experiment\n",
    "page_rank_results = run_experiment(function=page_rank_container, experiment=page_rank_experiment)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Results\n",
    "\n",
    "FaaS Runner experiment results are parsed into a Pandas dataframe. This flexibility allows the ability to perform any kind of data processing that you would like."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import matplotlib and setup display.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Histogram of runtime\n",
    "plt.hist(hello_world_results['userRuntime'], 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import matplotlib and setup display.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Histogram of runtime\n",
    "plt.hist(page_rank_results['userRuntime'], 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 6: Expand to Multiple Platform\n",
    "\n",
    "SAAF functions and cloud functions in this notebook support multiple platforms. Currently functions can be deployed to AWS Lambda, Google Cloud Functions, and IBM Cloud Functions. Azure Cloud Functions should work, but something about the deployment scripts broke. So Azure Functions support is coming in the future. The three supported platforms support all the same features of AWS Lambda functions with a few limitations. \n",
    " 1. If a function is deployed to multiple platforms (such as the one below) then only the LOCAL run mode is supported. If a single platform is selected then the CLOUD run mode is supported on all platforms.\n",
    " 2. Google Cloud Functions take a while to deploy and regularly need to be deployed more than once to get running. To deploy a function multiple times run this cell, wait for the processing to finish, make a minor change to the code (like adding a space at the end of a line) and run the cell again.\n",
    " 3. Containerize is only supported on AWS Lambda.\n",
    "\n",
    "Run the cell below to deploy this cloud function to all supported platforms. After the functions are deployed and working, run the experiment below to compare the runtime of each platform."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@cloud_function(platforms=[Platform.AWS, Platform.GCF, Platform.IBM], runMode=RunMode.LOCAL)\n",
    "def hello_faas(request, context): \n",
    "    from Inspector import Inspector\n",
    "    inspector = Inspector()\n",
    "    inspector.inspectAll()\n",
    "    inspector.addAttribute(\"message\", \"Hello from \" + inspector.getAttribute(\"platform\") +  \" \" + str(request[\"name\"]) + \"!\")\n",
    "    inspector.inspectAllDeltas()\n",
    "    return inspector.finish()\n",
    "\n",
    "hello_faas({\"name\": \"Bob\"}, None) # What platform would this call? Idk, so multiplatform functions only support RunMode.LOCAL\n",
    "\n",
    "# So instead use test to call your function on each platform...\n",
    "test(function=hello_faas, payload={\"name\": \"Bob\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Or run experiments on each platform. Here we define a simple experiment.\n",
    "platform_experiment = {\n",
    "  \"payloads\": [{\"name\": \"Bob\"}],\n",
    "  \"memorySettings\": [],\n",
    "  \"runs\": 10,\n",
    "  \"threads\": 10,\n",
    "  \"iterations\": 1,\n",
    "  \"warmupBuffer\": 0,\n",
    "  \"sleepTime\": 0,\n",
    "  \"randomSeed\": 42,\n",
    "  \"showAsList\": [],\n",
    "  \"showAsSum\": [\"newcontainer\"],\n",
    "  \"ignoreFromAll\": [\"zAll\", \"version\", \"linuxVersion\", \"hostname\"],\n",
    "  \"invalidators\": {},\n",
    "  \"removeDuplicateContainers\": False,\n",
    "  \"overlapFilter\": \"functionName\",\n",
    "  \"openCSV\": False\n",
    "}\n",
    "\n",
    "# Then run the experiments on each platform\n",
    "lambdaResults = run_experiment(function=hello_faas, experiment=platform_experiment, platform=Platform.AWS)\n",
    "googleResults = run_experiment(function=hello_faas, experiment=platform_experiment, platform=Platform.GCF)\n",
    "ibmResults = run_experiment(function=hello_faas, experiment=platform_experiment, platform=Platform.IBM)\n",
    "\n",
    "# And find out the average runtime of each platform\n",
    "print(\"AWS Lambda Average Runtime: \" + str(np.average(lambdaResults['runtime'])))\n",
    "print(\"Google Cloud Functions Average Runtime: \" + str(np.average(googleResults['runtime'])))\n",
    "print(\"IBM Cloud Functions Average Runtime: \" + str(np.average(ibmResults['runtime'])))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}